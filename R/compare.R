# _________________________________Roxygen_________________________________________________________
#' Comparing two samples by measuring their similarity
#'
#' It returns spatial similarity of two data sets with equal number of columns (variables) and any number
#' of rows (observations).
#'
#' @param smpl1 First smaple as a matrix or data frame object
#' @param smpl2 Second sample
#' @param n_ Number by which each dimension (column) is divided (default 5)
#' @param par_ Whether or not to use parallelism (defaults TRUE)
#'
#' @details TBD
#'
#' @return Similarity value
#'
#' @seealso \code{\link{clustering}} for clustering a similarity matrix generated by \code{compare}.
#'
#' @author Morteza H. Chalabi, \url{mor.chalabi@@gmail.com}
#'
#' @examples
#' require(compaRe)
#' require(dbscan)
#'
#' rm(list = ls())
#'
#' # Step 1: Reading in fcs files
#'
#' data(package = 'compaRe', list = c('dataset1','dataset2'))
#'
#' # Step 2: Transforming data
#'
#' smpl1[ smpl1 < 0 ] = 0      # negative values in a fcs files are noise
#' smpl1 = log(smpl1+1)        # fcs data are logarithmically distributed
#' smpl2[ smpl2 < 0 ] = 0
#' smpl2 = log(smpl2+1)
#'
#' # Step 3: Comparing (measuring similarity)
#'
#' message(compaRe::compare(smpl1 = smpl1, smpl2 = smpl2, n_ = 4))
#'
#' @export

compare = function(smpl1 = NULL, smpl2 = NULL, n_ = 5, par_ = TRUE)
{
  require(parallel)
  require(dbscan)     # for LOF

  # STEP 1: Preprocessing ####
  message('Preprocessing')

  # universal resizing so both samples are within range [-1,1] in all dimensions
  smpl_tmp = do.call(what = rbind, args = list(smpl1, smpl2))
  smpl_tmp = 2*(smpl_tmp - min(smpl_tmp))/diff(range(smpl_tmp))-1
  smpl1 = smpl_tmp[1:nrow(smpl1),]
  smpl2 = smpl_tmp[(nrow(smpl1)+1):nrow(smpl_tmp),]
  smpls_ = list(smpl1, smpl2)
  rm(smpl1, smpl2)

  # STEP 2: Forming hypercubes ####
  message('Forming hypercubes')

  rgns_smpl = list()      # a list to store regions (hypercubes) for both samples
  i_ = 1                  # counter of rgns_smpl
  dvds_ = c(seq(-1,1,length.out = n_+1)[1:n_],1.01)     # dividing points
  for(smpl_i in smpls_)
  {
    rgns_ = rep(0, nrow(smpl_i))      # holds region numbers for each child node
    tot_rgns = 0
    for(d_ in 1:ncol(smpl_i))
    {
      s_d_1 = (rgns_-tot_rgns) %/% n_^(d_-2)          # number of siblings of the parent node (at d-1 dimension). rgns is branch number of the parent
      f_d_1 = (rgns_-tot_rgns) - s_d_1*n_^(d_-2)      # number of families before the parent node
      f_d = f_d_1*n_ + s_d_1                          # number of family before current child node

      # dividing current division
      for(j_ in 2:length(dvds_))
      {
        rows_ = which(dvds_[j_-1] <= smpl_i[,d_] & smpl_i[,d_] < dvds_[j_])
        if( 0 < length(rows_))
        {
          rgns_[rows_] = (tot_rgns + n_^(d_-1)) + f_d[rows_] + (j_-2)*n_^(d_-1)
        }
      }

      # updating total regions (hypercubes)
      tot_rgns = tot_rgns + n_^(d_-1)
    }
    rgns_smpl[[i_]] = rgns_
    i_ = i_+1
  }

  # STEP 3 : Measuring similarity ####
  message('Measuring similarity\n\tRemoving outliers')

  # dissimilarity value in a hypercube: subtraction of 2 portions if common, 1 if exclusive
  common_rgns = intersect(rgns_smpl[[1]],rgns_smpl[[2]])      # intersection of regions in both samples common regions
  myFunc = function(i_, rgns_smpl, smpls_)
  {
    # computing LOF on points in exclusive regions
    exlv_rgns_disim = 0                                           # holds amount of dissimilarity in exclusive regions
    exlv_rgns_inds = which(!rgns_smpl[[i_]] %in% common_rgns)     # indices of exclusive regions
    if(length(exlv_rgns_inds) != 0)
    {
      exlv_rgns = rgns_smpl[[i_]][exlv_rgns_inds]             # exclusive regions
      exlv_pnts = smpls_[[i_]][exlv_rgns_inds,,drop = F]      # points in exclusive regions
      if(20 <= nrow(exlv_pnts))                               # perform LOF if there are at least 20 exclusive points
      {
        lof_ = dbscan::lof(x = exlv_pnts, k = 3, approx = round(nrow(exlv_pnts)/1e5,1))     # kNN works too slowly if dataset size is over 1e5. approx helps to speed up
        exlv_rgns_disim = length(unique(exlv_rgns[which(lof_ <= 1.2)]))                               # dissim in exclusive regions: for each signal exclusive region (i.e. lof <= 1.2), 1 is incremented as penalty
      }

      # extracting common regions
      rgns_smpl[[i_]] = rgns_smpl[[i_]][-exlv_rgns_inds]
    }
    comm_rgns_disim = table(sort(rgns_smpl[[i_]]))/nrow(smpls_[[i_]])     # dissim in common regions

    return(list(exlv_rgns_disim = exlv_rgns_disim, comm_rgns_disim = comm_rgns_disim))
  }
  if(par_)
  {
    cl_ = makeCluster(getOption("cl.cores", 2))
    dissim_ls = parLapply(X = 1:2, fun = myFunc, rgns_smpl, smpls_, cl = cl_)
    stopCluster(cl_)
  }else
  {
    dissim_ls = lapply(X = 1:2, FUN = myFunc, rgns_smpl, smpls_)
  }

  # measuring dissimilarity
  exlv_rgns_disim = dissim_ls[[1]]$exlv_rgns_disim + dissim_ls[[2]]$exlv_rgns_disim             # dissim in exclusive regions
  comm_rgns_disim = abs(dissim_ls[[1]]$comm_rgns_disim - dissim_ls[[2]]$comm_rgns_disim)        # dissim in common regions
  mean_dissim = sum(exlv_rgns_disim,comm_rgns_disim)/(exlv_rgns_disim+length(common_rgns))      # total dissim is average of dissim vals in common and exclusive regions
  return( (1-mean_dissim)*100)      # similarity is 1-dissimilarity
}
